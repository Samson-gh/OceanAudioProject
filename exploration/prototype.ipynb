{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import complexPyTorch as cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import the data_exp module\n",
    "from exploration.data_exp import explore_audio_file, load_audio, plot_waveform, plot_spectrogram, get_audio_features\n",
    "from exploration.data_loader import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data/audio\")\n",
    "audio_files = list(data_dir.glob(\"*.wav\")) + list(data_dir.glob(\"*.mp3\"))\n",
    "\n",
    "if not audio_files:\n",
    "    print(f\"No audio files found in {data_dir}\")\n",
    "    print(\"Please place your .wav or .mp3 files in this directory\")\n",
    "else:\n",
    "    print(f\"Found {len(audio_files)} audio files:\")\n",
    "    for file in audio_files:\n",
    "        print(f\"- {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in audio_files:\n",
    "    print(f\"\\nProcessing: {audio_file.name}\")\n",
    "    explore_audio_file(str(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the first audio file as an example\n",
    "if audio_files:\n",
    "    y, sr = load_audio(str(audio_files[0]))\n",
    "    \n",
    "    # Take a segment of the signal to analyze (complex wavelets can be computationally intensive)\n",
    "    segment_length = 1000  # analyze first 1000 samples\n",
    "    y_segment = y[:segment_length]\n",
    "    \n",
    "    # Perform continuous wavelet transform with cmor1.5-1.0\n",
    "    wavelet = 'cmor1.5-1.0'  # complex Morlet wavelet\n",
    "    scales = np.arange(1, 128)  # scales to analyze\n",
    "    \n",
    "    # Calculate the CWT\n",
    "    coef, freqs = pywt.cwt(y_segment, scales, wavelet)\n",
    "    \n",
    "    # Create time array for plotting\n",
    "    times = np.arange(len(y_segment)) / sr\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot original signal\n",
    "    ax1.plot(times, y_segment)\n",
    "    ax1.set_title('Original Signal Segment')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot scalogram (absolute values of CWT coefficients)\n",
    "    im = ax2.pcolormesh(times, freqs, np.abs(coef), shading='gouraud', cmap='jet')\n",
    "    ax2.set_title('Continuous Wavelet Transform\\n(Complex Morlet Wavelet: cmor1.5-1.0)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax2, label='Magnitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some information about the analysis\n",
    "    print(\"\\nWavelet Analysis Information:\")\n",
    "    print(f\"Wavelet type: {wavelet}\")\n",
    "    print(f\"Number of scales analyzed: {len(scales)}\")\n",
    "    print(f\"Signal segment length: {segment_length} samples ({segment_length/sr:.2f} seconds)\")\n",
    "    print(f\"Frequency range: {freqs[0]:.1f} - {freqs[-1]:.1f} Hz\")\n",
    "else:\n",
    "    print(\"No audio files available for wavelet analysis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cwt(x, B, C, fs):\n",
    "  wavelet = \"cmor\"+f\"{B}-{C}\"\n",
    "  widths = np.geomspace(1, 1024, num=100)\n",
    "  time = np.linspace(0, 3, 3 * fs, endpoint=False)\n",
    "  sampling_period = np.diff(time).mean()\n",
    "  cwtmatr, freqs = pywt.cwt(x, widths, wavelet, sampling_period=sampling_period)\n",
    "  return cwtmatr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data loading utilities\n",
    "from data_loader import create_dataloader\n",
    "\n",
    "# Define parameters for data loading\n",
    "data_dir = \"data/audio\"\n",
    "batch_size = 32\n",
    "sample_rate = 44100  # Standard audio sample rate\n",
    "duration = 3.0  # 3 second clips to match our CWT function\n",
    "\n",
    "# Create transform pipeline\n",
    "transform = torch.nn.Sequential(\n",
    "    # Normalize audio to [-1, 1] range\n",
    "    torch.nn.Lambda(lambda x: x / torch.max(torch.abs(x)))\n",
    ")\n",
    "\n",
    "# Create the data loader\n",
    "train_loader = create_dataloader(\n",
    "    data_dir=data_dir,\n",
    "    batch_size=batch_size,\n",
    "    sample_rate=sample_rate,\n",
    "    duration=duration,\n",
    "    #transform=transform,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Created DataLoader with {len(train_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CVNN, self).__init__()\n",
    "        self.layer1 = ComplexConv2d(1, embed_dim, kernel_size=(5,5), stride=(1,2), padding=1)\n",
    "        self.batchnorm1 = ComplexBatchNorm2d(embed_dim)\n",
    "        self.layer2 = ComplexConv2d(embed_dim, hidden_size, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.batchnorm2 = ComplexBatchNorm2d(hidden_size)\n",
    "        self.layer3 = ComplexLinear(1486016, hidden_size)\n",
    "        self.layer4 = ComplexLinear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = complex_relu(self.layer1(x))\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = complex_relu(self.layer2(x))\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = complex_relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        x = x.abs()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CVNN(input_dim=1, hidden_dim=64, output_dim=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cvnn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
